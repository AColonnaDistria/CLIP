{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train CLIP model on long flicker dataset (need to have done preprocessing)"
      ],
      "metadata": {
        "id": "Q9LVeaxldbJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg4hofwJfvXB",
        "outputId": "91b129d6-991e-45d8-8a1e-312a89603858"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLSADGsKhzdY",
        "outputId": "a74914b3-ac16-4c10-80f5-dc07da4fc661"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'gdrive'\n",
            "/content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "colab_file_dir='My Drive/Colab Notebooks' # where clip_helpers.py is for me\n",
        "sys.path.append(colab_file_dir)"
      ],
      "metadata": {
        "id": "HQwqfMe7f17Q"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import zipfile\n",
        "import requests\n",
        "import io\n",
        "import math\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import csv\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.utils import register_keras_serializable\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.metrics import Mean\n",
        "from tensorflow.keras.layers import Dropout, Rescaling\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Pour utiliser au mieux le GPU\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n"
      ],
      "metadata": {
        "id": "FUVrAhvzOwpN"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Classes à utiliser pour la partie classification de texte. Laquelle ?\n",
        "  ## --> pratique si on récupère le premier token 'CLS' style\n",
        "@register_keras_serializable()\n",
        "class SelectFirstToken (layers.Layer):\n",
        "  # Retourne le premier mot\n",
        "    def call(self, inputs):\n",
        "        return inputs[:, 0] # (batch, embed_dim)\n",
        "\n",
        "@register_keras_serializable()\n",
        "class SelectMean(layers.Layer):\n",
        "  # Retourne la moyenne des mots - bien si pas trop de PAD - chaînes même taille\n",
        "    def call(self, inputs):\n",
        "        # inputs: (batch, seq_len, embed_dim)\n",
        "        return tf.reduce_mean(inputs, axis=1)  # (batch, embed_dim)\n",
        "\n",
        "\n",
        "@register_keras_serializable()\n",
        "class MaskedMean(layers.Layer):\n",
        "  # Retourne la moyenne des mots sans être trop influencé par PAD\n",
        "    def call(self, inputs):\n",
        "        seq_out, token_ids = inputs   # (B,L,D), (B,L)\n",
        "        mask = tf.cast(tf.not_equal(token_ids, 0), seq_out.dtype)  # PAD=0\n",
        "        mask = tf.expand_dims(mask, -1)        # (B,L,1)\n",
        "        summed = tf.reduce_sum(seq_out * mask, axis=1)             # (B,D)\n",
        "        counts = tf.reduce_sum(mask, axis=1)                        # (B,1)\n",
        "        return summed / tf.maximum(counts, 1.0)\n",
        "\n",
        "\n",
        "# Classe utile pour la partie Clip mais il fallait bien regarder pour la trouver\n",
        "@register_keras_serializable()\n",
        "class L2Normalize(layers.Layer):\n",
        "    def __init__(self, axis=-1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.axis = axis\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.math.l2_normalize(inputs, axis=self.axis)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"axis\": self.axis})\n",
        "        return config\n",
        "\n",
        "# PARTIE SMALL_BERT = COPIE DU NOTEBOOK\n",
        "# ============================\n",
        "# PositionalEmbedding Layer\n",
        "# ============================\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(input_dim=vocab_size,\n",
        "                                                 output_dim=embed_dim)\n",
        "        self.position_embeddings = layers.Embedding(input_dim=sequence_length,\n",
        "                                                    output_dim=embed_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(0, length)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# ============================\n",
        "# TransformerBlock\n",
        "# ============================\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                             key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "        attn_mask = None\n",
        "        if mask is not None:\n",
        "            attn_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.float32)\n",
        "            attn_mask = tf.tile(attn_mask, [1, seq_len, 1])\n",
        "\n",
        "        attn_output = self.att(inputs, inputs, inputs, attention_mask=attn_mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        def get_config(self):\n",
        "            config = super().get_config()\n",
        "            config.update({\n",
        "                \"embed_dim\": self.att.key_dim,\n",
        "                \"num_heads\": self.att.num_heads,\n",
        "                \"ff_dim\": self.ffn.layers[0].units,\n",
        "                \"dropout_rate\": self.dropout1.rate,\n",
        "            })\n",
        "            return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "# ============================\n",
        "# SmallBERT encoder\n",
        "# ============================\n",
        "@register_keras_serializable()\n",
        "class SmallBERT(tf.keras.Model):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, num_heads,\n",
        "                 ff_dim, num_layers, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_embedding = PositionalEmbedding(sequence_length, vocab_size,\n",
        "                                                 embed_dim)\n",
        "\n",
        "        self.transformer_blocks = [\n",
        "              TransformerBlock(embed_dim,\n",
        "                             num_heads, ff_dim) for _ in range(num_layers)\n",
        "        ]\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.pos_embedding(inputs)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, training=training)\n",
        "        x = self.layernorm(x)\n",
        "        return self.dropout(x, training=training)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "            \"num_layers\": self.num_layers,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "# Perte contrastive CLIP\n",
        "# Le but de cette fonction est d’aligner les embeddings d’images et de textes\n",
        "# correspondants dans un espace latent partgé. Elle est inspirée du papier\n",
        "# CLIP, où l'on entraîne le modèle à prédire quelle image correspond à quel\n",
        "# texte et réciproquement.\n",
        "\n",
        "@register_keras_serializable(package=\"clip\")\n",
        "class ClipLossLayer(layers.Layer):\n",
        "    def __init__(self, temperature=0.14, **kwargs):\n",
        "        # inital temp was 0.07\n",
        "        super().__init__(**kwargs)\n",
        "        self.temperature = temperature\n",
        "        self.clip_loss_metric = tf.keras.metrics.Mean(name=\"clip_loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # l'inputs est forcément (img, txt) ou [img, txt]\n",
        "        img, txt = inputs  # (B, D) attention il faut avoir L2-normalisés !!\n",
        "\n",
        "        # Matrice des similarités (cosinus parce qu'on a L2 zt ça simplifie)\n",
        "        logits = tf.matmul(img, txt, transpose_b=True) / self.temperature\n",
        "\n",
        "        # Les Labels implicites : c'est la diagonale\n",
        "        labels = tf.range(tf.shape(logits)[0])\n",
        "\n",
        "        li = tf.keras.losses.sparse_categorical_crossentropy(labels,\n",
        "                                                             logits,\n",
        "                                                             from_logits=True)\n",
        "        lt = tf.keras.losses.sparse_categorical_crossentropy(labels,\n",
        "                                                          tf.transpose(logits),\n",
        "                                                          from_logits=True)\n",
        "        loss = tf.reduce_mean(li + lt) / 2.0\n",
        "\n",
        "        # Ca c'est super important car on ajoute la loss au graphe\n",
        "        # du modèle et ça nous simplifie la vie\n",
        "        # après on met à jour la métrique interne si on veut la suivre\n",
        "        self.add_loss(loss)\n",
        "        self.clip_loss_metric.update_state(loss)\n",
        "\n",
        "        # On retourne un TUPLE de tenseurs(surtout pas une liste)\n",
        "        # dc facileà récupérer\n",
        "        return (img, txt)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {**super().get_config(), \"temperature\": self.temperature}"
      ],
      "metadata": {
        "id": "Ba54FTQbO-rM"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history_simple(history):\n",
        "    \"\"\"\n",
        "    Trace côte à côte les courbes Loss et Accuracy (train/val si dispo)\n",
        "    à partir d'un objet Keras History.\n",
        "    \"\"\"\n",
        "    hist = history.history\n",
        "\n",
        "    # compatibilité anciennes versions (\"acc\"/\"val_acc\")\n",
        "    acc_key = \"accuracy\" if \"accuracy\" in hist else \"acc\"\n",
        "    val_acc_key = \"val_accuracy\" if \"val_accuracy\" in hist else \"val_acc\"\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    # --- Loss ---\n",
        "    if \"loss\" in hist:\n",
        "        axes[0].plot(hist[\"loss\"], label=\"train\")\n",
        "    if \"val_loss\" in hist:\n",
        "        axes[0].plot(hist[\"val_loss\"], label=\"val\")\n",
        "    axes[0].set_title(\"Loss\")\n",
        "    axes[0].set_xlabel(\"Epoch\")\n",
        "    axes[0].set_ylabel(\"Loss\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    # --- Accuracy ---\n",
        "    if acc_key in hist:\n",
        "        axes[1].plot(hist[acc_key], label=\"train\")\n",
        "        if val_acc_key in hist:\n",
        "            axes[1].plot(hist[val_acc_key], label=\"val\")\n",
        "        axes[1].set_title(\"Accuracy\")\n",
        "        axes[1].set_xlabel(\"Epoch\")\n",
        "        axes[1].set_ylabel(\"Accuracy\")\n",
        "        axes[1].legend()\n",
        "    else:\n",
        "        axes[1].set_visible(False)  # si pas d'accuracy, on masque le 2e plot\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def save_history_csv(history, filename):\n",
        "    \"\"\"save history as csv file\"\"\"\n",
        "    history_dict = history.history\n",
        "    keys = history_dict.keys()  # headers\n",
        "    rows = zip(*history_dict.values())  # values\n",
        "\n",
        "    with open(filename, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(keys)\n",
        "        writer.writerows(rows)"
      ],
      "metadata": {
        "id": "NIvJzQv6PDP6"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load parameters"
      ],
      "metadata": {
        "id": "q7NSMttQkPPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_params(\n",
        "        dataset_dir=\"./flickr_long_subset\",\n",
        "        model_dir=\"./models_forclip\") -> dict:\n",
        "\n",
        "    # returns a dictionary with all the parameters\n",
        "    # parameters can be modified\n",
        "    train_dir = os.path.join(dataset_dir, \"train_data\")\n",
        "    val_dir = os.path.join(dataset_dir, \"val_data\")\n",
        "    test_dir = os.path.join(dataset_dir, \"test_data\")\n",
        "\n",
        "    train_image_dir = os.path.join(train_dir, \"images\")\n",
        "    train_captions_dir = os.path.join(train_dir, \"captions\")\n",
        "    train_captions_csv_path = os.path.join(train_dir, \"captions.csv\")\n",
        "\n",
        "    val_image_dir = os.path.join(val_dir, \"images\")\n",
        "    val_captions_dir = os.path.join(val_dir, \"captions\")\n",
        "    val_captions_csv_path = os.path.join(val_dir, \"captions.csv\")\n",
        "\n",
        "    test_image_dir = os.path.join(test_dir, \"images\")\n",
        "    test_captions_dir = os.path.join(test_dir, \"captions\")\n",
        "    test_captions_csv_path = os.path.join(test_dir, \"captions.csv\")\n",
        "\n",
        "    # token form train dataset\n",
        "    vocab_path = os.path.join(dataset_dir, \"vocab.txt\")\n",
        "\n",
        "    image_size = (224, 224)\n",
        "\n",
        "    param_dict = {\n",
        "        # directories for data and model weights\n",
        "        \"model_dir\": model_dir,\n",
        "        \"dataset_dir\": dataset_dir,\n",
        "\n",
        "        \"train_dir\": train_dir,\n",
        "        \"train_image_dir\": train_image_dir,\n",
        "        \"train_captions_dir\": train_captions_dir,\n",
        "        \"train_captions_csv_path\": train_captions_csv_path,\n",
        "\n",
        "        \"val_dir\": val_dir,\n",
        "        \"val_image_dir\": val_image_dir,\n",
        "        \"val_captions_dir\": val_captions_dir,\n",
        "        \"val_captions_csv_path\": val_captions_csv_path,\n",
        "\n",
        "        \"test_dir\": test_dir,\n",
        "        \"test_image_dir\": test_image_dir,\n",
        "        \"test_captions_dir\": test_captions_dir,\n",
        "        \"test_captions_csv_path\": test_captions_csv_path,\n",
        "\n",
        "        \"vocab_path\": vocab_path,\n",
        "\n",
        "        # model training parameters\n",
        "        # Attention respecter bien l'ordre alphabétique des classes pour\n",
        "        # le générateur\n",
        "        \"class_names\": ['ball', 'bike', 'dog', 'water'],\n",
        "        # class encoding dict\n",
        "        \"class_dict\": {\n",
        "            \"ball\": 0,\n",
        "            \"bike\": 1,\n",
        "            \"dog\": 2,\n",
        "            \"water\": 3\n",
        "        },\n",
        "        # Pour les images\n",
        "        \"image_size\": image_size,\n",
        "        \"image_shape\": image_size + (3,),\n",
        "\n",
        "\n",
        "        # Pour les textes\n",
        "        \"sequence_length\": 32,\n",
        "        \"vocab_size\": 10000,\n",
        "        \"num_heads\": 4,\n",
        "        \"ff_dim\": 128,\n",
        "        \"num_layers\": 2,\n",
        "        \"nb_image_filters\": 32,\n",
        "        \"pad_sequence\": True,\n",
        "\n",
        "        # Pour les images et les textes dans le modèle CLIP\n",
        "        \"embed_dim\": 128,\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"data_augmentation\": True,\n",
        "\n",
        "        # pour le training:\n",
        "        \"patience\": 10,\n",
        "        \"batch_size\": 64,\n",
        "        \"nb_epochs\": 30,\n",
        "    }\n",
        "\n",
        "    return param_dict\n"
      ],
      "metadata": {
        "id": "3ZWJsV87PHTr"
      },
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your directories here\n",
        "dataset_dir = \"My Drive/ML2_projet/flickr_long_subset\"  # diretory with train/val/test folders\n",
        "model_dir = \"My Drive/ML2_projet/models_forclip\"\n",
        "\n",
        "param_dict = get_default_params(\n",
        "    dataset_dir=dataset_dir,\n",
        "    model_dir=model_dir\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "3n6aNiKVfrEJ"
      },
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build, train and save model"
      ],
      "metadata": {
        "id": "MMN26YkOkR8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA LOADER\n",
        "def make_clip_dataset(\n",
        "        dataset_dir,\n",
        "        captions_csv_path,\n",
        "        tokenizer_layer,\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        drop_remainder=True,\n",
        "        cache=True,\n",
        "        seed=42):\n",
        "\n",
        "    \"\"\"\n",
        "    Construit un tf.data.Dataset avec en sortie (images, token_ids) pour CLIP.\n",
        "    \"\"\"\n",
        "    # On récupère le fichier captions.csv qui a tout\n",
        "    df = pd.read_csv(captions_csv_path)\n",
        "    image_paths = df[\"image_path\"].astype(str).tolist()\n",
        "    captions = df[\"caption\"].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "    # Récupération du répertoire des images\n",
        "    root = Path(dataset_dir)\n",
        "    full_paths = [str(root / p) for p in image_paths]\n",
        "\n",
        "    # Création du dataset d'image\n",
        "    ds = tf.data.Dataset.from_tensor_slices((full_paths, captions))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(full_paths), seed=seed,\n",
        "                        reshuffle_each_iteration=True)\n",
        "\n",
        "    # Chargement d'une ensemble d'images normalisées et de tokens (le texte)\n",
        "    IMAGE_H, IMAGE_W = 224, 224  # même que image_size\n",
        "\n",
        "    def load_sample(img_path, caption):\n",
        "        img = tf.io.read_file(img_path)\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "        img = tf.image.resize(img, [IMAGE_H, IMAGE_W])\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)  # [0,1]\n",
        "        tokens = tf.cast(tokenizer_layer(caption), tf.int32)  # (L,)\n",
        "\n",
        "        # x = dict des 2 entrées, pour forcer à ne pas avoir de  y\n",
        "        return {\"image_input\": img, \"text_input\": tokens}\n",
        "\n",
        "    # Pour utiliser le cache et pouvoir faire les traitements en //\n",
        "    ds = ds.map(load_sample, num_parallel_calls=AUTOTUNE)\n",
        "    if cache:\n",
        "        ds = ds.cache()\n",
        "    # si drop_remainder=True on vire le dernier batch\n",
        "    # s'il n'est pas de la bonne taille\n",
        "    ds = ds.batch(batch_size, drop_remainder=drop_remainder).prefetch(AUTOTUNE)\n",
        "    return ds"
      ],
      "metadata": {
        "id": "0NoTSfplPMnl"
      },
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL BUILDER\n",
        "def build_clip_model(\n",
        "        sequence_length,\n",
        "        vocab_size,\n",
        "        embed_dim,\n",
        "        num_attention_heads,\n",
        "        ff_dim,\n",
        "        num_layers,\n",
        "        nb_image_filters=32,\n",
        "        pad_sequence: bool = True,\n",
        "        compile=True,\n",
        "        learning_rate=1e-4,\n",
        "        data_augmentation=True):\n",
        "\n",
        "    # Part 1: Text encoding\n",
        "    # Text inputs (need layer name to match tf.dataset keys)\n",
        "    # Text inputs are already tokenized\n",
        "    text_inputs = layers.Input(shape=(sequence_length,), dtype=tf.int32, name=\"text_input\")\n",
        "\n",
        "    # smallBertEncoding\n",
        "    # attention mask will be determined by the bert model\n",
        "    base_model = SmallBERT(\n",
        "        sequence_length=sequence_length,\n",
        "        vocab_size=vocab_size,\n",
        "        embed_dim=embed_dim,\n",
        "        num_heads=num_attention_heads,\n",
        "        ff_dim=ff_dim,\n",
        "        num_layers=num_layers\n",
        "    )(text_inputs)\n",
        "\n",
        "    # Final vector representation\n",
        "    # how to compress information to a single vector\n",
        "    # text_vector = MaskedMean()((base_model, text_inputs))\n",
        "    text_vector = MaskedMean()((base_model, text_inputs))\n",
        "\n",
        "    # add some dropout to see if it helps with overtraining\n",
        "    text_vector_droupout = layers.Dropout(0.2)(text_vector)\n",
        "\n",
        "    # normalize\n",
        "    norm_text_vector = L2Normalize(name=\"text_latent_vector\")(text_vector_droupout)\n",
        "\n",
        "    print(\"start building image model\")\n",
        "    # Part 2: Image encoding\n",
        "    # Image inputs (need layer name to match tf.dataset keys)\n",
        "    image_inputs = tf.keras.Input(shape=(224, 224, 3), name='image_input')\n",
        "\n",
        "    # Image processing\n",
        "    rescaling_layer = Rescaling(1./255)(image_inputs)\n",
        "\n",
        "    # data augmentation\n",
        "    # randlomly change image data a little\n",
        "    if (data_augmentation):\n",
        "        # flip image randolmy\n",
        "        # test horizontal and versical maybe ?\n",
        "        flipLayer = layers.RandomFlip(\n",
        "            mode=\"horizontal\")(rescaling_layer)\n",
        "\n",
        "        # rotate image randomly\n",
        "        rotationLayer = layers.RandomRotation(\n",
        "            factor=0.2,  # can rotate up to 20% of a full rotation left and right\n",
        "            )(flipLayer)\n",
        "\n",
        "        # Zoom randomly\n",
        "        zoomLayer = layers.RandomZoom(\n",
        "            height_factor=(0.2, 0.2),  # can zoom in and out to 20% and keep image ratio\n",
        "            )(rotationLayer)\n",
        "\n",
        "        # still training too fast, need to add additional layers of\n",
        "        # Randomly change brightness and contrast\n",
        "        contrastLayer = layers.RandomContrast(0.4)(zoomLayer)\n",
        "\n",
        "        # # Still overfitting after 3-4 epochs\n",
        "        brightnessLayer = layers.RandomBrightness(0.1)(contrastLayer)\n",
        "\n",
        "        # Slightly better but still overfitting\n",
        "        translationLayer = layers.RandomTranslation(0.1,0.1)(brightnessLayer)\n",
        "\n",
        "        x = translationLayer\n",
        "    else:\n",
        "        x = rescaling_layer\n",
        "    # CNN encoding\n",
        "    convolution_layer1 = Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    max_pooling_layer1 = MaxPooling2D(pool_size=(2, 2))(convolution_layer1)\n",
        "\n",
        "    convolution_layer2 = Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(max_pooling_layer1)\n",
        "    max_pooling_layer2 = MaxPooling2D(pool_size=(2, 2))(convolution_layer2)\n",
        "\n",
        "    convolution_layer3 = Conv2D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_regularizer=regularizers.l2(0.001))(max_pooling_layer2)\n",
        "    max_pooling_layer3 = MaxPooling2D(pool_size=(2, 2))(convolution_layer3)\n",
        "\n",
        "    flatten_layer = Flatten()(max_pooling_layer3)\n",
        "    # set image latent space size equal to text latent space size\n",
        "    image_vector = Dense(\n",
        "        embed_dim,\n",
        "        activation=\"relu\")(flatten_layer)\n",
        "\n",
        "    # Normalize\n",
        "    norm_image_vector = L2Normalize(name=\"image_latent_vector\")(image_vector)\n",
        "\n",
        "    # CLIP part\n",
        "    print(\"start building clip part\")\n",
        "    # Compute clip distance\n",
        "    clip_layer = ClipLossLayer(name=\"clip_loss_layer\")([norm_image_vector, norm_text_vector])\n",
        "\n",
        "    # Final part: build model\n",
        "    print(\"build full model\")\n",
        "    # final model\n",
        "    final_model = keras.Model(inputs=[image_inputs, text_inputs], outputs=clip_layer, name=\"clip_training\")\n",
        "    final_model.summary()\n",
        "\n",
        "    if compile:\n",
        "        print(\"compile\")\n",
        "        # No loss in compile, it's in the model --> I AM BLIND THIS WAS THE ANSWER ALL ALONG..\n",
        "        final_model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        )\n",
        "    return final_model"
      ],
      "metadata": {
        "id": "WBaVrkDxPR71"
      },
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN AND SAVE\n",
        "def train_model(\n",
        "        model_config: dict,  # dict with all params (loaded from load_default_params),\n",
        "        model_name: str,  # basename of the model\n",
        "        ):\n",
        "\n",
        "    # Train and save the model weights to given location\n",
        "\n",
        "    full_model_name = \"_\".join([\n",
        "        model_name,\n",
        "        str(model_config['nb_epochs']),\n",
        "        \"epochs\"\n",
        "    ])\n",
        "\n",
        "    tokenizer = TextVectorization(\n",
        "        max_tokens=model_config['vocab_size'],\n",
        "        standardize='lower_and_strip_punctuation',\n",
        "        split='whitespace',\n",
        "        vocabulary=model_config[\"vocab_path\"],\n",
        "        pad_to_max_tokens=model_config[\"pad_sequence\"],\n",
        "        output_sequence_length=model_config[\"sequence_length\"],\n",
        "        output_mode=\"int\"  # save 0 for pad tokens\n",
        "        )\n",
        "\n",
        "    train_ds = make_clip_dataset(\n",
        "        model_config[\"train_dir\"],\n",
        "        model_config[\"train_captions_csv_path\"],\n",
        "        tokenizer,\n",
        "        batch_size=model_config[\"batch_size\"])\n",
        "\n",
        "    val_ds = make_clip_dataset(\n",
        "        model_config[\"val_dir\"],\n",
        "        model_config[\"val_captions_csv_path\"],\n",
        "        tokenizer,\n",
        "        batch_size=model_config[\"batch_size\"])\n",
        "\n",
        "    model = build_clip_model(\n",
        "        sequence_length=model_config[\"sequence_length\"],\n",
        "        vocab_size=model_config[\"vocab_size\"],\n",
        "        embed_dim=model_config[\"embed_dim\"],\n",
        "        num_attention_heads=model_config[\"num_heads\"],\n",
        "        ff_dim=model_config[\"ff_dim\"],\n",
        "        num_layers=model_config[\"num_layers\"],\n",
        "        nb_image_filters=model_config[\"nb_image_filters\"],\n",
        "        pad_sequence=model_config[\"pad_sequence\"],\n",
        "        learning_rate=model_config[\"learning_rate\"],\n",
        "        data_augmentation=model_config[\"data_augmentation\"]\n",
        "    )\n",
        "\n",
        "    save_dir = model_config[\"model_dir\"]\n",
        "    name_model = f\"{full_model_name}.weights.h5\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    model_path = os.path.join(save_dir, name_model)\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor=\"val_clip_loss\", mode=\"min\", patience=model_config[\"patience\"],\n",
        "                      restore_best_weights=True),\n",
        "        ModelCheckpoint(model_path, monitor=\"val_clip_loss\", mode=\"min\",\n",
        "                        save_best_only=True, save_weights_only=True),\n",
        "        # tensorboard_callback\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=model_config[\"nb_epochs\"],\n",
        "        verbose=1,\n",
        "        callbacks=callbacks)\n",
        "\n",
        "    # save history\n",
        "    history_filename = os.path.join(save_dir, f\"{full_model_name}_history.csv\")\n",
        "    save_history_csv(history, history_filename)\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "ZAikQ2kQPVqR"
      },
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIrst iteration is longer than the others\n",
        "\n",
        "history = train_model(\n",
        "    param_dict,\n",
        "    \"clip_pipeline_model_big_dataset_betterCNN_data_augmentation_bs64_val_loss_patience5_epoch30_latent128_textDrop0.2_3ConvLayer_lessAugmentation_dropoutTransformer_0.2&_lerningrate2-4_temp0.14\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aRIniJyBkKbY",
        "outputId": "8c45876c-6a2a-49f7-8029-c8eb044d39ca"
      },
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start building image model\n",
            "start building clip part\n",
            "build full model\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"clip_training\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"clip_training\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ rescaling_47        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mRescaling\u001b[0m)         │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ random_flip_43      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ rescaling_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mRandomFlip\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ random_rotation_43  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ random_flip_43[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mRandomRotation\u001b[0m)    │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ random_zoom_43      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ random_rotation_… │\n",
              "│ (\u001b[38;5;33mRandomZoom\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ random_contrast_15  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ random_zoom_43[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mRandomContrast\u001b[0m)    │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ random_brightness_… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ random_contrast_… │\n",
              "│ (\u001b[38;5;33mRandomBrightness\u001b[0m)  │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ random_translation… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ random_brightnes… │\n",
              "│ (\u001b[38;5;33mRandomTranslation\u001b[0m) │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_128 (\u001b[38;5;33mConv2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │        \u001b[38;5;34m896\u001b[0m │ random_translati… │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_128   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_128[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_129 (\u001b[38;5;33mConv2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_12… │\n",
              "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_129   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_129[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_130 (\u001b[38;5;33mConv2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    │     \u001b[38;5;34m73,856\u001b[0m │ max_pooling2d_12… │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_130   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_130[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ small_bert_47       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m1,879,040\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mSmallBERT\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten_47          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100352\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_13… │\n",
              "│ (\u001b[38;5;33mFlatten\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ masked_mean_47      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ small_bert_47[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mMaskedMean\u001b[0m)        │                   │            │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_239 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │ \u001b[38;5;34m12,845,184\u001b[0m │ flatten_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_355         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ masked_mean_47[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ image_latent_vector │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_239[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mL2Normalize\u001b[0m)       │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text_latent_vector  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_355[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mL2Normalize\u001b[0m)       │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ clip_loss_layer     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │          \u001b[38;5;34m0\u001b[0m │ image_latent_vec… │\n",
              "│ (\u001b[38;5;33mClipLossLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │ text_latent_vect… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ rescaling_47        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ random_flip_43      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ rescaling_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomFlip</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ random_rotation_43  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ random_flip_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomRotation</span>)    │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ random_zoom_43      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ random_rotation_… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomZoom</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ random_contrast_15  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ random_zoom_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomContrast</span>)    │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ random_brightness_… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ random_contrast_… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomBrightness</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ random_translation… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ random_brightnes… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomTranslation</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_128 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ random_translati… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_128   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_128[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_129 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_12… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_129   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_129[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_130 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ max_pooling2d_12… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_130   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_130[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ small_bert_47       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,879,040</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SmallBERT</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten_47          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100352</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_13… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ masked_mean_47      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ small_bert_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaskedMean</span>)        │                   │            │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_239 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">12,845,184</span> │ flatten_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_355         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ masked_mean_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ image_latent_vector │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_239[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">L2Normalize</span>)       │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text_latent_vector  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_355[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">L2Normalize</span>)       │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ clip_loss_layer     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_latent_vec… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ClipLossLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │ text_latent_vect… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,817,472\u001b[0m (56.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,817,472</span> (56.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,817,472\u001b[0m (56.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,817,472</span> (56.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compile\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'masked_mean_47' (of type MaskedMean) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 753ms/step - clip_loss: 4.2062 - loss: 4.3386 - val_clip_loss: 4.1609 - val_loss: 4.2895\n",
            "Epoch 2/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 267ms/step - clip_loss: 4.1693 - loss: 4.2965 - val_clip_loss: 4.1603 - val_loss: 4.2830\n",
            "Epoch 3/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 508ms/step - clip_loss: 4.1662 - loss: 4.2874 - val_clip_loss: 4.1598 - val_loss: 4.2765\n",
            "Epoch 4/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 240ms/step - clip_loss: 4.1630 - loss: 4.2782 - val_clip_loss: 4.1597 - val_loss: 4.2704\n",
            "Epoch 5/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 271ms/step - clip_loss: 4.1612 - loss: 4.2705 - val_clip_loss: 4.1595 - val_loss: 4.2644\n",
            "Epoch 6/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 279ms/step - clip_loss: 4.1618 - loss: 4.2654 - val_clip_loss: 4.1595 - val_loss: 4.2589\n",
            "Epoch 7/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 270ms/step - clip_loss: 4.1601 - loss: 4.2581 - val_clip_loss: 4.1595 - val_loss: 4.2535\n",
            "Epoch 8/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - clip_loss: 4.1604 - loss: 4.2533 - val_clip_loss: 4.1594 - val_loss: 4.2484\n",
            "Epoch 9/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step - clip_loss: 4.1599 - loss: 4.2478 - val_clip_loss: 4.1594 - val_loss: 4.2437\n",
            "Epoch 10/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 252ms/step - clip_loss: 4.1598 - loss: 4.2430 - val_clip_loss: 4.1594 - val_loss: 4.2391\n",
            "Epoch 11/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 265ms/step - clip_loss: 4.1594 - loss: 4.2382 - val_clip_loss: 4.1593 - val_loss: 4.2348\n",
            "Epoch 12/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 569ms/step - clip_loss: 4.1597 - loss: 4.2342 - val_clip_loss: 4.1593 - val_loss: 4.2308\n",
            "Epoch 13/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 239ms/step - clip_loss: 4.1597 - loss: 4.2303 - val_clip_loss: 4.1593 - val_loss: 4.2270\n",
            "Epoch 14/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - clip_loss: 4.1593 - loss: 4.2261 - val_clip_loss: 4.1593 - val_loss: 4.2234\n",
            "Epoch 15/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 611ms/step - clip_loss: 4.1596 - loss: 4.2229 - val_clip_loss: 4.1592 - val_loss: 4.2200\n",
            "Epoch 16/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step - clip_loss: 4.1592 - loss: 4.2192 - val_clip_loss: 4.1592 - val_loss: 4.2169\n",
            "Epoch 17/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step - clip_loss: 4.1595 - loss: 4.2163 - val_clip_loss: 4.1592 - val_loss: 4.2139\n",
            "Epoch 18/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 229ms/step - clip_loss: 4.1597 - loss: 4.2136 - val_clip_loss: 4.1592 - val_loss: 4.2111\n",
            "Epoch 19/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 594ms/step - clip_loss: 4.1591 - loss: 4.2104 - val_clip_loss: 4.1591 - val_loss: 4.2084\n",
            "Epoch 20/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 253ms/step - clip_loss: 4.1590 - loss: 4.2076 - val_clip_loss: 4.1591 - val_loss: 4.2059\n",
            "Epoch 21/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 227ms/step - clip_loss: 4.1597 - loss: 4.2059 - val_clip_loss: 4.1591 - val_loss: 4.2036\n",
            "Epoch 22/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 609ms/step - clip_loss: 4.1584 - loss: 4.2023 - val_clip_loss: 4.1591 - val_loss: 4.2014\n",
            "Epoch 23/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 418ms/step - clip_loss: 4.1588 - loss: 4.2005 - val_clip_loss: 4.1591 - val_loss: 4.1993\n",
            "Epoch 24/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 229ms/step - clip_loss: 4.1597 - loss: 4.1995 - val_clip_loss: 4.1590 - val_loss: 4.1974\n",
            "Epoch 25/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - clip_loss: 4.1603 - loss: 4.1982 - val_clip_loss: 4.1590 - val_loss: 4.1956\n",
            "Epoch 26/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - clip_loss: 4.1597 - loss: 4.1958 - val_clip_loss: 4.1591 - val_loss: 4.1940\n",
            "Epoch 27/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step - clip_loss: 4.1591 - loss: 4.1935 - val_clip_loss: 4.1591 - val_loss: 4.1924\n",
            "Epoch 28/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step - clip_loss: 4.1593 - loss: 4.1922 - val_clip_loss: 4.1591 - val_loss: 4.1909\n",
            "Epoch 29/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 146ms/step - clip_loss: 4.1592 - loss: 4.1907 - val_clip_loss: 4.1591 - val_loss: 4.1895\n",
            "Epoch 30/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - clip_loss: 4.1593 - loss: 4.1893 - val_clip_loss: 4.1591 - val_loss: 4.1881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history_simple(history)"
      ],
      "metadata": {
        "id": "v2hJ30Tjo-CP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "50c9eae4-366d-4888-be9c-42875b2a7605"
      },
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}