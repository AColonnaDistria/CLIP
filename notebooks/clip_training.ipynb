{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km3IQstSWTcD",
        "outputId": "3e4eb9af-6042-41c3-b511-6aedf2dc3511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "my_local_drive='/content/gdrive/MyDrive/ML2_projet'\n",
        "sys.path.append(my_local_drive)"
      ],
      "metadata": {
        "id": "hKWJ_-ZFWbN5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $my_local_drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeiK98a8We6C",
        "outputId": "8081aa1c-1f82-4364-c3bb-3a28fa480f50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ML2_projet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import zipfile\n",
        "import requests\n",
        "import io\n",
        "import math\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.utils import register_keras_serializable\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.metrics import Mean\n",
        "from tensorflow.keras.layers import Dropout, Rescaling\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Pour utiliser au mieux le GPU\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "0z8UUsDrWhJt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorboard initialisation\n",
        "%load_ext tensorboard\n",
        "log_dir = my_local_drive + \"/logs/\" + datetime.datetime.now().strftime(\"%d-%m-%Y-%H%M%S\")\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "sys.path.append(log_dir)\n",
        "%tensorboard --logdir $log_dir\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "aZgws3HBosmY",
        "outputId": "6d6b73c8-3a4a-4ca3-85df-1df2a28295b9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Launching TensorBoard..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[Errno 107] Transport endpoint is not connected",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4077561615.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensorboard'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--logdir $log_dir'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorboard/notebook.py\u001b[0m in \u001b[0;36m_start_magic\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_start_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;34m\"\"\"Implementation of the `%tensorboard` line magic.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorboard/notebook.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(args_string)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mparsed_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mstart_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStartLaunched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorboard/manager.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(arguments, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     this_cache_key = cache_key(\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mworking_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mconfigure_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 107] Transport endpoint is not connected"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classes à utiliser pour la partie classification de texte. Laquelle ?\n",
        "  ## --> pratique si on récupère le premier token 'CLS' style\n",
        "@register_keras_serializable()\n",
        "class SelectFirstToken (layers.Layer):\n",
        "  # Retourne le premier mot\n",
        "    def call(self, inputs):\n",
        "        return inputs[:, 0] # (batch, embed_dim)\n",
        "\n",
        "@register_keras_serializable()\n",
        "class SelectMean(layers.Layer):\n",
        "  # Retourne la moyenne des mots - bien si pas trop de PAD - chaînes même taille\n",
        "    def call(self, inputs):\n",
        "        # inputs: (batch, seq_len, embed_dim)\n",
        "        return tf.reduce_mean(inputs, axis=1)  # (batch, embed_dim)\n",
        "\n",
        "\n",
        "@register_keras_serializable()\n",
        "class MaskedMean(layers.Layer):\n",
        "  # Retourne la moyenne des mots sans être trop influencé par PAD\n",
        "    def call(self, inputs):\n",
        "        seq_out, token_ids = inputs   # (B,L,D), (B,L)\n",
        "        mask = tf.cast(tf.not_equal(token_ids, 0), seq_out.dtype)  # PAD=0\n",
        "        mask = tf.expand_dims(mask, -1)        # (B,L,1)\n",
        "        summed = tf.reduce_sum(seq_out * mask, axis=1)             # (B,D)\n",
        "        counts = tf.reduce_sum(mask, axis=1)                        # (B,1)\n",
        "        return summed / tf.maximum(counts, 1.0)\n",
        "\n",
        "\n",
        "# Classe utile pour la partie Clip mais il fallait bien regarder pour la trouver\n",
        "@register_keras_serializable()\n",
        "class L2Normalize(layers.Layer):\n",
        "    def __init__(self, axis=-1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.axis = axis\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.math.l2_normalize(inputs, axis=self.axis)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"axis\": self.axis})\n",
        "        return config\n",
        "\n",
        "# PARTIE SMALL_BERT = COPIE DU NOTEBOOK\n",
        "# ============================\n",
        "# PositionalEmbedding Layer\n",
        "# ============================\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(input_dim=vocab_size,\n",
        "                                                 output_dim=embed_dim)\n",
        "        self.position_embeddings = layers.Embedding(input_dim=sequence_length,\n",
        "                                                    output_dim=embed_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(0, length)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# ============================\n",
        "# TransformerBlock\n",
        "# ============================\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads,ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                             key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "        attn_mask = None\n",
        "        if mask is not None:\n",
        "            attn_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.float32)\n",
        "            attn_mask = tf.tile(attn_mask, [1, seq_len, 1])\n",
        "\n",
        "        attn_output = self.att(inputs, inputs, inputs, attention_mask=attn_mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        def get_config(self):\n",
        "            config = super().get_config()\n",
        "            config.update({\n",
        "                \"embed_dim\": self.att.key_dim,\n",
        "                \"num_heads\": self.att.num_heads,\n",
        "                \"ff_dim\": self.ffn.layers[0].units,\n",
        "                \"dropout_rate\": self.dropout1.rate,\n",
        "            })\n",
        "            return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "# ============================\n",
        "# SmallBERT encoder\n",
        "# ============================\n",
        "@register_keras_serializable()\n",
        "class SmallBERT(tf.keras.Model):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, num_heads,\n",
        "                 ff_dim, num_layers, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_embedding = PositionalEmbedding(sequence_length, vocab_size,\n",
        "                                                 embed_dim)\n",
        "\n",
        "        self.transformer_blocks = [\n",
        "              TransformerBlock(embed_dim,\n",
        "                             num_heads, ff_dim) for _ in range(num_layers)\n",
        "        ]\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.pos_embedding(inputs)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, training=training)\n",
        "        x = self.layernorm(x)\n",
        "        return self.dropout(x, training=training)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "            \"num_layers\": self.num_layers,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n"
      ],
      "metadata": {
        "id": "45pdQKGxWi3t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perte contrastive CLIP\n",
        "# Le but de cette fonction est d’aligner les embeddings d’images et de textes\n",
        "# correspondants dans un espace latent partgé. Elle est inspirée du papier\n",
        "# CLIP, où l'on entraîne le modèle à prédire quelle image correspond à quel\n",
        "# texte et réciproquement.\n",
        "\n",
        "@register_keras_serializable(package=\"clip\")\n",
        "class ClipLossLayer(layers.Layer):\n",
        "    def __init__(self, temperature=0.07, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.temperature = temperature\n",
        "        self.clip_loss_metric = tf.keras.metrics.Mean(name=\"clip_loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # l'inputs est forcément (img, txt) ou [img, txt]\n",
        "        img, txt = inputs  # (B, D) attention il faut avoir L2-normalisés !!\n",
        "\n",
        "        # Matrice des similarités (cosinus parce qu'on a L2 zt ça simplifie)\n",
        "        logits = tf.matmul(img, txt, transpose_b=True) / self.temperature\n",
        "\n",
        "        # Les Labels implicites : c'est la diagonale\n",
        "        labels = tf.range(tf.shape(logits)[0])\n",
        "\n",
        "        li = tf.keras.losses.sparse_categorical_crossentropy(labels,\n",
        "                                                             logits,\n",
        "                                                             from_logits=True)\n",
        "        lt = tf.keras.losses.sparse_categorical_crossentropy(labels,\n",
        "                                                          tf.transpose(logits),\n",
        "                                                          from_logits=True)\n",
        "        loss = tf.reduce_mean(li + lt) / 2.0\n",
        "\n",
        "        # Ca c'est super important car on ajoute la loss au graphe\n",
        "        # du modèle et ça nous simplifie la vie\n",
        "        # après on met à jour la métrique interne si on veut la suivre\n",
        "        self.add_loss(loss)\n",
        "        self.clip_loss_metric.update_state(loss)\n",
        "\n",
        "        # On retourne un TUPLE de tenseurs(surtout pas une liste)\n",
        "        # dc facileà récupérer\n",
        "        return (img, txt)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {**super().get_config(), \"temperature\": self.temperature}\n"
      ],
      "metadata": {
        "id": "po_TV-_-5tJG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history_simple(history):\n",
        "    \"\"\"\n",
        "    Trace côte à côte les courbes Loss et Accuracy (train/val si dispo)\n",
        "    à partir d'un objet Keras History.\n",
        "    \"\"\"\n",
        "    hist = history.history\n",
        "\n",
        "    # compatibilité anciennes versions (\"acc\"/\"val_acc\")\n",
        "    acc_key = \"accuracy\" if \"accuracy\" in hist else \"acc\"\n",
        "    val_acc_key = \"val_accuracy\" if \"val_accuracy\" in hist else \"val_acc\"\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    # --- Loss ---\n",
        "    if \"loss\" in hist:\n",
        "        axes[0].plot(hist[\"loss\"], label=\"train\")\n",
        "    if \"val_loss\" in hist:\n",
        "        axes[0].plot(hist[\"val_loss\"], label=\"val\")\n",
        "    axes[0].set_title(\"Loss\")\n",
        "    axes[0].set_xlabel(\"Epoch\")\n",
        "    axes[0].set_ylabel(\"Loss\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    # --- Accuracy ---\n",
        "    if acc_key in hist:\n",
        "        axes[1].plot(hist[acc_key], label=\"train\")\n",
        "        if val_acc_key in hist:\n",
        "            axes[1].plot(hist[val_acc_key], label=\"val\")\n",
        "        axes[1].set_title(\"Accuracy\")\n",
        "        axes[1].set_xlabel(\"Epoch\")\n",
        "        axes[1].set_ylabel(\"Accuracy\")\n",
        "        axes[1].legend()\n",
        "    else:\n",
        "        axes[1].set_visible(False)  # si pas d'accuracy, on masque le 2e plot\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "XTPehgSIQ_qj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Répertoire cible pour sauvegarder vos modèles\n",
        "model_dir = \"./models_forclip\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "# Répertoire des données\n",
        "dataset_dir = \"./flickr_subset2\"\n",
        "train_dir = os.path.join(dataset_dir, \"train_data\")\n",
        "# Répertoire des images\n",
        "image_dir = os.path.join(train_dir, \"images\")\n",
        "# Répertoire des captions\n",
        "captions_dir = os.path.join(train_dir, \"captions\")\n",
        "\n",
        "# caption_path\n",
        "captions_csv_path = os.path.join(train_dir, \"captions.csv\")\n",
        "\n",
        "# token form train dataset\n",
        "vocab_path = os.path.join(dataset_dir, \"vocab.txt\")\n",
        "\n",
        "# Variables utiles\n",
        "# Attention respecter bien l'ordre alphabétique des classes pour\n",
        "# le générateur\n",
        "class_names = ['ball', 'bike', 'dog', 'water']\n",
        "# class encoding dict\n",
        "class_dict = {\n",
        "    \"ball\": 0,\n",
        "    \"bike\": 1,\n",
        "    \"dog\": 2,\n",
        "    \"water\": 3\n",
        "}\n",
        "# Pour les images\n",
        "image_size=(224, 224)\n",
        "image_shape = image_size + (3,)\n",
        "\n",
        "\n",
        "# Pour les textes\n",
        "sequence_length = 32\n",
        "vocab_size = 10000\n",
        "num_heads = 4\n",
        "ff_dim = 256\n",
        "num_layers = 2\n",
        "\n",
        "# Pour les images et les textes dans le modèle CLIP\n",
        "embed_dim = 128\n",
        "\n",
        "# pour le training:\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "xoJa5N9CWk0Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA LOADER"
      ],
      "metadata": {
        "id": "s32MeCHNT2ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_clip_dataset_smallbert(\n",
        "    captions_csv_path,\n",
        "    tokenizer_layer,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    drop_remainder=True,\n",
        "    cache=True,\n",
        "    seed=42,\n",
        "):\n",
        "    \"\"\"\n",
        "    Construit un tf.data.Dataset avec en sortie (images, token_ids) pour CLIP.\n",
        "    \"\"\"\n",
        "    # On récupère le fichier captions.csv qui a tout\n",
        "    df = pd.read_csv(captions_csv_path)\n",
        "    image_paths = df[\"image_path\"].astype(str).tolist()\n",
        "    captions    = df[\"caption\"].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "    # Récupération du répertoire des images\n",
        "    root = Path(dataset_dir)\n",
        "    full_paths = [str(root / p) for p in image_paths]\n",
        "\n",
        "    # Création du dataset d'image\n",
        "    ds = tf.data.Dataset.from_tensor_slices((full_paths, captions))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(full_paths), seed=seed,\n",
        "                        reshuffle_each_iteration=True)\n",
        "\n",
        "    # Chargement d'une ensemble d'images normalisées et de tokens (le texte)\n",
        "    IMAGE_H, IMAGE_W = 224, 224  # même que image_size\n",
        "\n",
        "    def load_sample(img_path, caption):\n",
        "        img = tf.io.read_file(img_path)\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "        img = tf.image.resize(img, [224, 224])\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)  # [0,1]\n",
        "        tokens = tf.cast(tokenizer_layer(caption), tf.int32)  # (L,)\n",
        "        # x = dict des 2 entrées, pour forcer à ne pas avoir de  y\n",
        "        return {\"image_input\": img, \"text_input\": tokens}\n",
        "\n",
        "\n",
        "    # Pour utiliser le cache et pouvoir faire les traitements en //\n",
        "    ds = ds.map(load_sample, num_parallel_calls=AUTOTUNE)\n",
        "    if cache:\n",
        "        ds = ds.cache()\n",
        "    # si drop_remainder=True on vire le dernier batch\n",
        "    # s'il n'est pas de la bonne taille\n",
        "    ds = ds.batch(batch_size, drop_remainder=drop_remainder).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# Quanq le tokenizer est initialisé, un exemple d'appel que vous pourrez faire :\n",
        "# train_dataset = make_clip_dataset_smallbert(captions_csv_path,\n",
        "#                                       text_tokenizer,\n",
        "#                                       batch_size=64)"
      ],
      "metadata": {
        "id": "i5H8IwK2T11K"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer adapting"
      ],
      "metadata": {
        "id": "dJk67jIPwgAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a token list file (one token per row)\n",
        "# Generate tokens on trian dataset only\n",
        "# def generate_token_file(tokenizer, train_ds, token_file_path:str, overwrite=False):\n",
        "#   if os.path.exists(token_file_path) and not overwrite:\n",
        "#     print(f\"token file already exist, skipping adapting: {token_file_path}\")\n",
        "#     return\n",
        "#   else:\n",
        "#     print(f\"generating token file: {token_file_path}\")\n",
        "#     tokenizer.adapt()\n",
        "#     with open(token_file_path, \"w\"):"
      ],
      "metadata": {
        "id": "B7OC7iDewfP4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL BUILDING"
      ],
      "metadata": {
        "id": "FkxR817lZoTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_clip_classifier(\n",
        "    sequence_length,\n",
        "    vocab_size,\n",
        "    embed_dim,\n",
        "    num_attention_heads,\n",
        "    ff_dim,\n",
        "    num_layers,\n",
        "    vocabulary,  # Vocabulary from a preadapted textvectorizer for text encoding\n",
        "    nb_image_filters=32,\n",
        "    training: bool=False,\n",
        "    pad_sequence: bool=True):\n",
        "\n",
        "\n",
        "  print(\"start building model\")\n",
        "  #### Part 1: Text encoding\n",
        "  ## Text inputs (need layer name to match tf.dataset keys)\n",
        "  ## Text inputs are already tokenized\n",
        "  text_inputs = layers.Input(shape=(sequence_length,), dtype=tf.int32, name=\"text_input\")\n",
        "\n",
        "  ## smallBertEncoding\n",
        "  # attention mask will be determined by the bert model\n",
        "  base_model = SmallBERT(\n",
        "      sequence_length=sequence_length,\n",
        "      vocab_size=vocab_size,\n",
        "      embed_dim=embed_dim,\n",
        "      num_heads=num_attention_heads,\n",
        "      ff_dim=ff_dim,\n",
        "      num_layers=num_layers\n",
        "  )(text_inputs)\n",
        "\n",
        "  ## Final vector representation\n",
        "  # how to compress information to a single vector\n",
        "  # text_vector = MaskedMean()((base_model, text_inputs))\n",
        "  text_vector = MaskedMean()((base_model, text_inputs))\n",
        "\n",
        "  # normalize\n",
        "  norm_text_vector = L2Normalize(name=\"text_latent_vector\")(text_vector)\n",
        "\n",
        "  print(\"start building image model\")\n",
        "  #### Part 2: Image encoding\n",
        "  ## Image inputs (need layer name to match tf.dataset keys)\n",
        "  image_inputs = tf.keras.Input(shape=(224, 224, 3), name='image_input')\n",
        "\n",
        "  ## Image processing\n",
        "  rescaling_layer = Rescaling(1./255)(image_inputs)\n",
        "\n",
        "  ## data augmentation (Sequential layer here ?)\n",
        "\n",
        "  ## CNN encoding\n",
        "  convolution_layer = Conv2D(\n",
        "      filters=nb_image_filters,\n",
        "      kernel_size=3,\n",
        "      activation=\"relu\")(rescaling_layer)\n",
        "  max_pooling_layer = MaxPooling2D(pool_size=(2, 2))(convolution_layer)\n",
        "  flatten_layer = Flatten()(max_pooling_layer)\n",
        "  ## set image latent space size equal to text latent space size\n",
        "  image_vector = Dense(embed_dim,\n",
        "                       activation=\"relu\")(flatten_layer)\n",
        "\n",
        "  ## Normalize\n",
        "  norm_image_vector = L2Normalize(name=\"image_latent_vector\")(image_vector)\n",
        "\n",
        "\n",
        "  ##### CLIP part\n",
        "  print(\"start building clip part\")\n",
        "  ## Compute clip distance\n",
        "  clip_layer = ClipLossLayer(name=\"clip_loss_layer\")([norm_image_vector, norm_text_vector])\n",
        "\n",
        "  ##### Final part: build model\n",
        "  print(\"build full model\")\n",
        "  # final model\n",
        "  final_model = keras.Model(inputs=[image_inputs, text_inputs], outputs=clip_layer, name=\"clip_training\")\n",
        "  final_model.summary()\n",
        "  print(\"compile\")\n",
        "  # No loss in compile, it's in the model --> I AM BLIND THIS WAS THE ANSWER ALL ALONG..\n",
        "  final_model.compile(\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    )\n",
        "  return final_model"
      ],
      "metadata": {
        "id": "SMDvggi0ZqiX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PIPELINE"
      ],
      "metadata": {
        "id": "FjdOevjQRMaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and train model\n",
        "model_config = {\n",
        "    \"sequence_length\":sequence_length,\n",
        "    \"vocab_size\":vocab_size,\n",
        "    \"embed_dim\":embed_dim,\n",
        "    \"num_attention_heads\":num_heads,\n",
        "    \"ff_dim\":ff_dim,\n",
        "    \"num_layers\":num_layers,\n",
        "    \"vocabulary\":vocab_path,\n",
        "    \"nb_image_filters\":32,\n",
        "    \"training\":True,\n",
        "    \"pad_sequence\":True\n",
        "}\n",
        "\n",
        "tokenizer = TextVectorization(\n",
        "        max_tokens=vocab_size,\n",
        "        standardize='lower_and_strip_punctuation',\n",
        "        split='whitespace',\n",
        "        vocabulary=model_config[\"vocabulary\"],\n",
        "        pad_to_max_tokens=model_config[\"pad_sequence\"],\n",
        "        output_sequence_length=model_config[\"sequence_length\"],\n",
        "        output_mode=\"int\"  # save 0 for pad tokens\n",
        "      )\n",
        "\n",
        "\n",
        "train_ds = make_clip_dataset_smallbert(captions_csv_path,\n",
        "                                      tokenizer,\n",
        "                                      batch_size=64)\n",
        "\n",
        "model = build_clip_classifier(**model_config)\n",
        "save_dir = model_dir\n",
        "name_model = \"first_test_clip_training.weights.h5\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "model_path = os.path.join(save_dir, name_model)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor=\"clip_loss\", mode=\"min\", patience=2,\n",
        "                  restore_best_weights=True),\n",
        "    ModelCheckpoint(model_path, monitor=\"clip_loss\", mode=\"max\",\n",
        "                    save_best_only=True, save_weights_only=True),\n",
        "    # tensorboard_callback\n",
        "]\n",
        "\n",
        "epochs=10\n",
        "history=model.fit(\n",
        "    train_ds,\n",
        "    # validation_data=ds_val,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "_SSg_M-NRSfj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e899091d-773b-4a92-e5b9-16740b6e7730"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start building model\n",
            "start building image model\n",
            "start building clip part\n",
            "build full model\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"clip_training\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"clip_training\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ rescaling_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mRescaling\u001b[0m)         │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m,  │        \u001b[38;5;34m896\u001b[0m │ rescaling_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m394272\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_2[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ small_bert_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m1,944,832\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mSmallBERT\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │ \u001b[38;5;34m50,466,944\u001b[0m │ flatten_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ masked_mean_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ small_bert_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mMaskedMean\u001b[0m)        │                   │            │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ image_latent_vector │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mL2Normalize\u001b[0m)       │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text_latent_vector  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ masked_mean_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mL2Normalize\u001b[0m)       │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ clip_loss_layer     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │          \u001b[38;5;34m0\u001b[0m │ image_latent_vec… │\n",
              "│ (\u001b[38;5;33mClipLossLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │ text_latent_vect… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ rescaling_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ rescaling_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">394272</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ small_bert_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,944,832</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SmallBERT</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">50,466,944</span> │ flatten_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ masked_mean_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ small_bert_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaskedMean</span>)        │                   │            │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ image_latent_vector │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">L2Normalize</span>)       │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text_latent_vector  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ masked_mean_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">L2Normalize</span>)       │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ clip_loss_layer     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_latent_vec… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ClipLossLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │ text_latent_vect… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m52,412,672\u001b[0m (199.94 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">52,412,672</span> (199.94 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m52,412,672\u001b[0m (199.94 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">52,412,672</span> (199.94 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compile\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'masked_mean_2' (of type MaskedMean) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 6s/step - clip_loss: 4.2118 - loss: 4.2118\n",
            "Epoch 2/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 5s/step - clip_loss: 4.1316 - loss: 4.1316\n",
            "Epoch 3/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 4s/step - clip_loss: 4.1057 - loss: 4.1057\n",
            "Epoch 4/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 3s/step - clip_loss: 4.0828 - loss: 4.0828\n",
            "Epoch 5/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4s/step - clip_loss: 4.0327 - loss: 4.0327\n",
            "Epoch 6/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4s/step - clip_loss: 3.9255 - loss: 3.9255\n",
            "Epoch 7/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 4s/step - clip_loss: 3.6138 - loss: 3.6138\n",
            "Epoch 8/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 5s/step - clip_loss: 2.8072 - loss: 2.8072\n",
            "Epoch 9/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 4s/step - clip_loss: 2.3085 - loss: 2.3085\n",
            "Epoch 10/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3s/step - clip_loss: 1.7380 - loss: 1.7380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history_simple(history)"
      ],
      "metadata": {
        "id": "ZoJF_B8sQ1j9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "d8ae60b5-de76-4db2-eea8-0a1dadd076ac"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "joVLzW2ZQ8wK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}