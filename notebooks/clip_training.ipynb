{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km3IQstSWTcD",
        "outputId": "b1c2bb1d-84af-45f5-d370-6b7bf795bd6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "my_local_drive='/content/gdrive/MyDrive/ML2_projet'\n",
        "sys.path.append(my_local_drive)"
      ],
      "metadata": {
        "id": "hKWJ_-ZFWbN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $my_local_drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeiK98a8We6C",
        "outputId": "f1fc0d29-23df-4d87-fa0d-1c5b27776217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ML2_projet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import zipfile\n",
        "import requests\n",
        "import io\n",
        "import math\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.utils import register_keras_serializable\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.metrics import Mean\n",
        "from tensorflow.keras.layers import Dropout, Rescaling\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Pour utiliser au mieux le GPU\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "0z8UUsDrWhJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorboard initialisation\n",
        "# %load_ext tensorboard\n",
        "# log_dir = my_local_drive + \"/logs/\" + datetime.datetime.now().strftime(\"%d-%m-%Y-%H%M%S\")\n",
        "# os.makedirs(log_dir, exist_ok=True)\n",
        "# sys.path.append(log_dir)\n",
        "# %tensorboard --logdir $log_dir\n",
        "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "aZgws3HBosmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classes à utiliser pour la partie classification de texte. Laquelle ?\n",
        "  ## --> pratique si on récupère le premier token 'CLS' style\n",
        "@register_keras_serializable()\n",
        "class SelectFirstToken (layers.Layer):\n",
        "  # Retourne le premier mot\n",
        "    def call(self, inputs):\n",
        "        return inputs[:, 0] # (batch, embed_dim)\n",
        "\n",
        "@register_keras_serializable()\n",
        "class SelectMean(layers.Layer):\n",
        "  # Retourne la moyenne des mots - bien si pas trop de PAD - chaînes même taille\n",
        "    def call(self, inputs):\n",
        "        # inputs: (batch, seq_len, embed_dim)\n",
        "        return tf.reduce_mean(inputs, axis=1)  # (batch, embed_dim)\n",
        "\n",
        "\n",
        "@register_keras_serializable()\n",
        "class MaskedMean(layers.Layer):\n",
        "  # Retourne la moyenne des mots sans être trop influencé par PAD\n",
        "    def call(self, inputs):\n",
        "        seq_out, token_ids = inputs   # (B,L,D), (B,L)\n",
        "        mask = tf.cast(tf.not_equal(token_ids, 0), seq_out.dtype)  # PAD=0\n",
        "        mask = tf.expand_dims(mask, -1)        # (B,L,1)\n",
        "        summed = tf.reduce_sum(seq_out * mask, axis=1)             # (B,D)\n",
        "        counts = tf.reduce_sum(mask, axis=1)                        # (B,1)\n",
        "        return summed / tf.maximum(counts, 1.0)\n",
        "\n",
        "\n",
        "# Classe utile pour la partie Clip mais il fallait bien regarder pour la trouver\n",
        "@register_keras_serializable()\n",
        "class L2Normalize(layers.Layer):\n",
        "    def __init__(self, axis=-1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.axis = axis\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.math.l2_normalize(inputs, axis=self.axis)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"axis\": self.axis})\n",
        "        return config\n",
        "\n",
        "# PARTIE SMALL_BERT = COPIE DU NOTEBOOK\n",
        "# ============================\n",
        "# PositionalEmbedding Layer\n",
        "# ============================\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(input_dim=vocab_size,\n",
        "                                                 output_dim=embed_dim)\n",
        "        self.position_embeddings = layers.Embedding(input_dim=sequence_length,\n",
        "                                                    output_dim=embed_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(0, length)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# ============================\n",
        "# TransformerBlock\n",
        "# ============================\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads,ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                             key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "        attn_mask = None\n",
        "        if mask is not None:\n",
        "            attn_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.float32)\n",
        "            attn_mask = tf.tile(attn_mask, [1, seq_len, 1])\n",
        "\n",
        "        attn_output = self.att(inputs, inputs, inputs, attention_mask=attn_mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        def get_config(self):\n",
        "            config = super().get_config()\n",
        "            config.update({\n",
        "                \"embed_dim\": self.att.key_dim,\n",
        "                \"num_heads\": self.att.num_heads,\n",
        "                \"ff_dim\": self.ffn.layers[0].units,\n",
        "                \"dropout_rate\": self.dropout1.rate,\n",
        "            })\n",
        "            return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "# ============================\n",
        "# SmallBERT encoder\n",
        "# ============================\n",
        "@register_keras_serializable()\n",
        "class SmallBERT(tf.keras.Model):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, num_heads,\n",
        "                 ff_dim, num_layers, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_embedding = PositionalEmbedding(sequence_length, vocab_size,\n",
        "                                                 embed_dim)\n",
        "\n",
        "        self.transformer_blocks = [\n",
        "              TransformerBlock(embed_dim,\n",
        "                             num_heads, ff_dim) for _ in range(num_layers)\n",
        "        ]\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.pos_embedding(inputs)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, training=training)\n",
        "        x = self.layernorm(x)\n",
        "        return self.dropout(x, training=training)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"ff_dim\": self.ff_dim,\n",
        "            \"num_layers\": self.num_layers,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n"
      ],
      "metadata": {
        "id": "45pdQKGxWi3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perte contrastive CLIP\n",
        "# Le but de cette fonction est d’aligner les embeddings d’images et de textes\n",
        "# correspondants dans un espace latent partgé. Elle est inspirée du papier\n",
        "# CLIP, où l'on entraîne le modèle à prédire quelle image correspond à quel\n",
        "# texte et réciproquement.\n",
        "\n",
        "@register_keras_serializable(package=\"clip\")\n",
        "class ClipLossLayer(layers.Layer):\n",
        "    def __init__(self, temperature=0.07, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.temperature = temperature\n",
        "        self.clip_loss_metric = tf.keras.metrics.Mean(name=\"clip_loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # l'inputs est forcément (img, txt) ou [img, txt]\n",
        "        img, txt = inputs  # (B, D) attention il faut avoir L2-normalisés !!\n",
        "\n",
        "        # Matrice des similarités (cosinus parce qu'on a L2 zt ça simplifie)\n",
        "        logits = tf.matmul(img, txt, transpose_b=True) / self.temperature\n",
        "\n",
        "        # Les Labels implicites : c'est la diagonale\n",
        "        labels = tf.range(tf.shape(logits)[0])\n",
        "\n",
        "        li = tf.keras.losses.sparse_categorical_crossentropy(labels,\n",
        "                                                             logits,\n",
        "                                                             from_logits=True)\n",
        "        lt = tf.keras.losses.sparse_categorical_crossentropy(labels,\n",
        "                                                          tf.transpose(logits),\n",
        "                                                          from_logits=True)\n",
        "        loss = tf.reduce_mean(li + lt) / 2.0\n",
        "\n",
        "        # Ca c'est super important car on ajoute la loss au graphe\n",
        "        # du modèle et ça nous simplifie la vie\n",
        "        # après on met à jour la métrique interne si on veut la suivre\n",
        "        self.add_loss(loss)\n",
        "        self.clip_loss_metric.update_state(loss)\n",
        "\n",
        "        # On retourne un TUPLE de tenseurs(surtout pas une liste)\n",
        "        # dc facileà récupérer\n",
        "        return (img, txt)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {**super().get_config(), \"temperature\": self.temperature}\n"
      ],
      "metadata": {
        "id": "po_TV-_-5tJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history_simple(history):\n",
        "    \"\"\"\n",
        "    Trace côte à côte les courbes Loss et Accuracy (train/val si dispo)\n",
        "    à partir d'un objet Keras History.\n",
        "    \"\"\"\n",
        "    hist = history.history\n",
        "\n",
        "    # compatibilité anciennes versions (\"acc\"/\"val_acc\")\n",
        "    acc_key = \"accuracy\" if \"accuracy\" in hist else \"acc\"\n",
        "    val_acc_key = \"val_accuracy\" if \"val_accuracy\" in hist else \"val_acc\"\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    # --- Loss ---\n",
        "    if \"loss\" in hist:\n",
        "        axes[0].plot(hist[\"loss\"], label=\"train\")\n",
        "    if \"val_loss\" in hist:\n",
        "        axes[0].plot(hist[\"val_loss\"], label=\"val\")\n",
        "    axes[0].set_title(\"Loss\")\n",
        "    axes[0].set_xlabel(\"Epoch\")\n",
        "    axes[0].set_ylabel(\"Loss\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    # --- Accuracy ---\n",
        "    if acc_key in hist:\n",
        "        axes[1].plot(hist[acc_key], label=\"train\")\n",
        "        if val_acc_key in hist:\n",
        "            axes[1].plot(hist[val_acc_key], label=\"val\")\n",
        "        axes[1].set_title(\"Accuracy\")\n",
        "        axes[1].set_xlabel(\"Epoch\")\n",
        "        axes[1].set_ylabel(\"Accuracy\")\n",
        "        axes[1].legend()\n",
        "    else:\n",
        "        axes[1].set_visible(False)  # si pas d'accuracy, on masque le 2e plot\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "XTPehgSIQ_qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Répertoire cible pour sauvegarder vos modèles\n",
        "model_dir = \"./models_forclip\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "# Répertoire des données\n",
        "dataset_dir = \"./flickr_subset2\"\n",
        "train_dir = os.path.join(dataset_dir, \"train_data\")\n",
        "# Répertoire des images\n",
        "image_dir = os.path.join(train_dir, \"images\")\n",
        "# Répertoire des captions\n",
        "captions_dir = os.path.join(train_dir, \"captions\")\n",
        "\n",
        "# caption_path\n",
        "captions_csv_path = os.path.join(train_dir, \"captions.csv\")\n",
        "\n",
        "# token form train dataset\n",
        "vocab_path = os.path.join(dataset_dir, \"vocab.txt\")\n",
        "\n",
        "# Variables utiles\n",
        "# Attention respecter bien l'ordre alphabétique des classes pour\n",
        "# le générateur\n",
        "class_names = ['ball', 'bike', 'dog', 'water']\n",
        "# class encoding dict\n",
        "class_dict = {\n",
        "    \"ball\": 0,\n",
        "    \"bike\": 1,\n",
        "    \"dog\": 2,\n",
        "    \"water\": 3\n",
        "}\n",
        "# Pour les images\n",
        "image_size=(224, 224)\n",
        "image_shape = image_size + (3,)\n",
        "\n",
        "\n",
        "# Pour les textes\n",
        "sequence_length = 32\n",
        "vocab_size = 10000\n",
        "num_heads = 4\n",
        "ff_dim = 256\n",
        "num_layers = 2\n",
        "\n",
        "# Pour les images et les textes dans le modèle CLIP\n",
        "embed_dim = 128\n",
        "\n",
        "# pour le training:\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "xoJa5N9CWk0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA LOADER"
      ],
      "metadata": {
        "id": "s32MeCHNT2ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_clip_dataset_smallbert(\n",
        "    captions_csv_path,\n",
        "    tokenizer_layer,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    drop_remainder=True,\n",
        "    cache=True,\n",
        "    seed=42,\n",
        "):\n",
        "    \"\"\"\n",
        "    Construit un tf.data.Dataset avec en sortie (images, token_ids) pour CLIP.\n",
        "    \"\"\"\n",
        "    # On récupère le fichier captions.csv qui a tout\n",
        "    df = pd.read_csv(captions_csv_path)\n",
        "    image_paths = df[\"image_path\"].astype(str).tolist()\n",
        "    captions    = df[\"caption\"].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "    # Récupération du répertoire des images\n",
        "    root = Path(dataset_dir)\n",
        "    full_paths = [str(root / p) for p in image_paths]\n",
        "\n",
        "    # Création du dataset d'image\n",
        "    ds = tf.data.Dataset.from_tensor_slices((full_paths, captions))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(full_paths), seed=seed,\n",
        "                        reshuffle_each_iteration=True)\n",
        "\n",
        "    # Chargement d'une ensemble d'images normalisées et de tokens (le texte)\n",
        "    IMAGE_H, IMAGE_W = 224, 224  # même que image_size\n",
        "\n",
        "    def load_sample(img_path, caption):\n",
        "        img = tf.io.read_file(img_path)\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "        img = tf.image.resize(img, [224, 224])\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)  # [0,1]\n",
        "        tokens = tf.cast(tokenizer_layer(caption), tf.int32)  # (L,)\n",
        "        # x = dict des 2 entrées, pour forcer à ne pas avoir de  y\n",
        "        return {\"image_input\": img, \"text_input\": tokens}\n",
        "\n",
        "\n",
        "    # Pour utiliser le cache et pouvoir faire les traitements en //\n",
        "    ds = ds.map(load_sample, num_parallel_calls=AUTOTUNE)\n",
        "    if cache:\n",
        "        ds = ds.cache()\n",
        "    # si drop_remainder=True on vire le dernier batch\n",
        "    # s'il n'est pas de la bonne taille\n",
        "    ds = ds.batch(batch_size, drop_remainder=drop_remainder).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# Quanq le tokenizer est initialisé, un exemple d'appel que vous pourrez faire :\n",
        "# train_dataset = make_clip_dataset_smallbert(captions_csv_path,\n",
        "#                                       text_tokenizer,\n",
        "#                                       batch_size=64)"
      ],
      "metadata": {
        "id": "i5H8IwK2T11K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer adapting"
      ],
      "metadata": {
        "id": "dJk67jIPwgAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a token list file (one token per row)\n",
        "# Generate tokens on trian dataset only\n",
        "# def generate_token_file(tokenizer, train_ds, token_file_path:str, overwrite=False):\n",
        "#   if os.path.exists(token_file_path) and not overwrite:\n",
        "#     print(f\"token file already exist, skipping adapting: {token_file_path}\")\n",
        "#     return\n",
        "#   else:\n",
        "#     print(f\"generating token file: {token_file_path}\")\n",
        "#     tokenizer.adapt()\n",
        "#     with open(token_file_path, \"w\"):"
      ],
      "metadata": {
        "id": "B7OC7iDewfP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL BUILDING"
      ],
      "metadata": {
        "id": "FkxR817lZoTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_clip_classifier(\n",
        "    sequence_length,\n",
        "    vocab_size,\n",
        "    embed_dim,\n",
        "    num_attention_heads,\n",
        "    ff_dim,\n",
        "    num_layers,\n",
        "    vocabulary,  # Vocabulary from a preadapted textvectorizer for text encoding\n",
        "    nb_image_filters=32,\n",
        "    training: bool=False,\n",
        "    pad_sequence: bool=True):\n",
        "\n",
        "import csv\n",
        "\n",
        "def save_history_csv(history, filename):\n",
        "    \"\"\"Save tf.keras History object to a CSV file.\"\"\"\n",
        "    hist_dict = history.history\n",
        "    keys = hist_dict.keys()\n",
        "    rows = zip(*hist_dict.values())\n",
        "\n",
        "    with open(filename, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(keys)\n",
        "        writer.writerows(rows)\n",
        "  print(\"start building model\")\n",
        "  #### Part 1: Text encoding\n",
        "  ## Text inputs (need layer name to match tf.dataset keys)\n",
        "  ## Text inputs are already tokenized\n",
        "  text_inputs = layers.Input(shape=(sequence_length,), dtype=tf.int32, name=\"text_input\")\n",
        "\n",
        "  ## smallBertEncoding\n",
        "  # attention mask will be determined by the bert model\n",
        "  base_model = SmallBERT(\n",
        "      sequence_length=sequence_length,\n",
        "      vocab_size=vocab_size,\n",
        "      embed_dim=embed_dim,\n",
        "      num_heads=num_attention_heads,\n",
        "      ff_dim=ff_dim,\n",
        "      num_layers=num_layers\n",
        "  )(text_inputs)\n",
        "\n",
        "  ## Final vector representation\n",
        "  # how to compress information to a single vector\n",
        "  # text_vector = MaskedMean()((base_model, text_inputs))\n",
        "  text_vector = MaskedMean()((base_model, text_inputs))\n",
        "\n",
        "  # normalize\n",
        "  norm_text_vector = L2Normalize(name=\"text_latent_vector\")(text_vector)\n",
        "\n",
        "  print(\"start building image model\")\n",
        "  #### Part 2: Image encoding\n",
        "  ## Image inputs (need layer name to match tf.dataset keys)\n",
        "  image_inputs = tf.keras.Input(shape=(224, 224, 3), name='image_input')\n",
        "\n",
        "  ## Image processing\n",
        "  rescaling_layer = Rescaling(1./255)(image_inputs)\n",
        "\n",
        "  ## data augmentation (Sequential layer here ?)\n",
        "\n",
        "  ## CNN encoding\n",
        "  convolution_layer = Conv2D(\n",
        "      filters=nb_image_filters,\n",
        "      kernel_size=3,\n",
        "      activation=\"relu\")(rescaling_layer)\n",
        "  max_pooling_layer = MaxPooling2D(pool_size=(2, 2))(convolution_layer)\n",
        "  flatten_layer = Flatten()(max_pooling_layer)\n",
        "  ## set image latent space size equal to text latent space size\n",
        "  image_vector = Dense(embed_dim,\n",
        "                       activation=\"relu\")(flatten_layer)\n",
        "\n",
        "  ## Normalize\n",
        "  norm_image_vector = L2Normalize(name=\"image_latent_vector\")(image_vector)\n",
        "\n",
        "\n",
        "  ##### CLIP part\n",
        "  print(\"start building clip part\")\n",
        "  ## Compute clip distance\n",
        "  clip_layer = ClipLossLayer(name=\"clip_loss_layer\")([norm_image_vector, norm_text_vector])\n",
        "\n",
        "  ##### Final part: build model\n",
        "  print(\"build full model\")\n",
        "  # final model\n",
        "  final_model = keras.Model(inputs=[image_inputs, text_inputs], outputs=clip_layer, name=\"clip_training\")\n",
        "  final_model.summary()\n",
        "  print(\"compile\")\n",
        "  # No loss in compile, it's in the model --> I AM BLIND THIS WAS THE ANSWER ALL ALONG..\n",
        "  final_model.compile(\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    )\n",
        "  return final_model"
      ],
      "metadata": {
        "id": "SMDvggi0ZqiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PIPELINE"
      ],
      "metadata": {
        "id": "FjdOevjQRMaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and train model\n",
        "model_config = {\n",
        "    \"sequence_length\":sequence_length,\n",
        "    \"vocab_size\":vocab_size,\n",
        "    \"embed_dim\":embed_dim,\n",
        "    \"num_attention_heads\":num_heads,\n",
        "    \"ff_dim\":ff_dim,\n",
        "    \"num_layers\":num_layers,\n",
        "    \"vocabulary\":vocab_path,\n",
        "    \"nb_image_filters\":32,\n",
        "    \"training\":True,\n",
        "    \"pad_sequence\":True\n",
        "}\n",
        "\n",
        "training_config = {\n",
        "    \"nb_epochs\":50,\n",
        "    \"batch_size\":16\n",
        "}\n",
        "\n",
        "base_model_name = \"clip_training\"\n",
        "\n",
        "full_model_name = \"_\".join([\n",
        "    base_model_name,\n",
        "    str(training_config['nb_epochs']),\n",
        "    \"epochs\"\n",
        "])\n",
        "\n",
        "\n",
        "tokenizer = TextVectorization(\n",
        "        max_tokens=vocab_size,\n",
        "        standardize='lower_and_strip_punctuation',\n",
        "        split='whitespace',\n",
        "        vocabulary=model_config[\"vocabulary\"],\n",
        "        pad_to_max_tokens=model_config[\"pad_sequence\"],\n",
        "        output_sequence_length=model_config[\"sequence_length\"],\n",
        "        output_mode=\"int\"  # save 0 for pad tokens\n",
        "      )\n",
        "\n",
        "\n",
        "train_ds = make_clip_dataset_smallbert(captions_csv_path,\n",
        "                                      tokenizer,\n",
        "                                      batch_size=training_config[\"batch_size\"])\n",
        "\n",
        "model = build_clip_classifier(**model_config)\n",
        "save_dir = model_dir\n",
        "name_model = f\"{full_model_name}.weights.h5\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "model_path = os.path.join(save_dir, name_model)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor=\"clip_loss\", mode=\"min\", patience=2,\n",
        "                  restore_best_weights=True),\n",
        "    ModelCheckpoint(model_path, monitor=\"clip_loss\", mode=\"min\",\n",
        "                    save_best_only=True, save_weights_only=True),\n",
        "    # tensorboard_callback\n",
        "]\n",
        "\n",
        "history=model.fit(\n",
        "    train_ds,\n",
        "    # validation_data=ds_val,\n",
        "    epochs=training_config[\"nb_epochs\"],\n",
        "    verbose=1,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "_SSg_M-NRSfj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "baf665c1-93bd-4d94-e59d-47602f7403c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start building model\n",
            "start building image model\n",
            "start building clip part\n",
            "build full model\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"clip_training\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"clip_training\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ rescaling           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mRescaling\u001b[0m)         │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m,  │        \u001b[38;5;34m896\u001b[0m │ rescaling[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m394272\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ small_bert          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m1,944,832\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mSmallBERT\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │ \u001b[38;5;34m50,466,944\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ masked_mean         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ small_bert[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│ (\u001b[38;5;33mMaskedMean\u001b[0m)        │                   │            │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ image_latent_vector │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mL2Normalize\u001b[0m)       │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text_latent_vector  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ masked_mean[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mL2Normalize\u001b[0m)       │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ clip_loss_layer     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │          \u001b[38;5;34m0\u001b[0m │ image_latent_vec… │\n",
              "│ (\u001b[38;5;33mClipLossLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │ text_latent_vect… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ rescaling           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ rescaling[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">394272</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ small_bert          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,944,832</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SmallBERT</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">50,466,944</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ masked_mean         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ small_bert[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaskedMean</span>)        │                   │            │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ image_latent_vector │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">L2Normalize</span>)       │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text_latent_vector  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ masked_mean[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">L2Normalize</span>)       │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ clip_loss_layer     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_latent_vec… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ClipLossLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │ text_latent_vect… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m52,412,672\u001b[0m (199.94 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">52,412,672</span> (199.94 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m52,412,672\u001b[0m (199.94 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">52,412,672</span> (199.94 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compile\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'masked_mean' (of type MaskedMean) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 7s/step - clip_loss: 2.8354 - loss: 2.8354\n",
            "Epoch 2/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 139ms/step - clip_loss: 2.7222 - loss: 2.7222\n",
            "Epoch 3/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - clip_loss: 2.5857 - loss: 2.5857\n",
            "Epoch 4/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - clip_loss: 1.8970 - loss: 1.8970\n",
            "Epoch 5/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 147ms/step - clip_loss: 1.1705 - loss: 1.1705\n",
            "Epoch 6/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 240ms/step - clip_loss: 0.5908 - loss: 0.5908\n",
            "Epoch 7/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 294ms/step - clip_loss: 0.2563 - loss: 0.2563\n",
            "Epoch 8/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step - clip_loss: 0.1297 - loss: 0.1297\n",
            "Epoch 9/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - clip_loss: 0.0792 - loss: 0.0792\n",
            "Epoch 10/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 425ms/step - clip_loss: 0.0564 - loss: 0.0564\n",
            "Epoch 11/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 156ms/step - clip_loss: 0.0487 - loss: 0.0487\n",
            "Epoch 12/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 326ms/step - clip_loss: 0.0376 - loss: 0.0376\n",
            "Epoch 13/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 392ms/step - clip_loss: 0.0330 - loss: 0.0330\n",
            "Epoch 14/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 229ms/step - clip_loss: 0.0293 - loss: 0.0293\n",
            "Epoch 15/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 327ms/step - clip_loss: 0.0264 - loss: 0.0264\n",
            "Epoch 16/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 196ms/step - clip_loss: 0.0231 - loss: 0.0231\n",
            "Epoch 17/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 320ms/step - clip_loss: 0.0229 - loss: 0.0229\n",
            "Epoch 18/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 172ms/step - clip_loss: 0.0200 - loss: 0.0200\n",
            "Epoch 19/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 204ms/step - clip_loss: 0.0183 - loss: 0.0183\n",
            "Epoch 20/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 134ms/step - clip_loss: 0.0169 - loss: 0.0169\n",
            "Epoch 21/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 381ms/step - clip_loss: 0.0166 - loss: 0.0166\n",
            "Epoch 22/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - clip_loss: 0.0151 - loss: 0.0151\n",
            "Epoch 23/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 211ms/step - clip_loss: 0.0137 - loss: 0.0137\n",
            "Epoch 24/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 644ms/step - clip_loss: 0.0133 - loss: 0.0133\n",
            "Epoch 25/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 267ms/step - clip_loss: 0.0131 - loss: 0.0131\n",
            "Epoch 26/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 196ms/step - clip_loss: 0.0128 - loss: 0.0128\n",
            "Epoch 27/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 179ms/step - clip_loss: 0.0119 - loss: 0.0119\n",
            "Epoch 28/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 133ms/step - clip_loss: 0.0116 - loss: 0.0116\n",
            "Epoch 29/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 168ms/step - clip_loss: 0.0109 - loss: 0.0109\n",
            "Epoch 30/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 135ms/step - clip_loss: 0.0107 - loss: 0.0107\n",
            "Epoch 31/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 238ms/step - clip_loss: 0.0101 - loss: 0.0101\n",
            "Epoch 32/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 254ms/step - clip_loss: 0.0095 - loss: 0.0095\n",
            "Epoch 33/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 325ms/step - clip_loss: 0.0089 - loss: 0.0089\n",
            "Epoch 34/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 146ms/step - clip_loss: 0.0088 - loss: 0.0088\n",
            "Epoch 35/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - clip_loss: 0.0083 - loss: 0.0083\n",
            "Epoch 36/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 137ms/step - clip_loss: 0.0080 - loss: 0.0080\n",
            "Epoch 37/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - clip_loss: 0.0079 - loss: 0.0079\n",
            "Epoch 38/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 262ms/step - clip_loss: 0.0078 - loss: 0.0078\n",
            "Epoch 39/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 132ms/step - clip_loss: 0.0074 - loss: 0.0074\n",
            "Epoch 40/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - clip_loss: 0.0072 - loss: 0.0072\n",
            "Epoch 41/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - clip_loss: 0.0082 - loss: 0.0082\n",
            "Epoch 42/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 225ms/step - clip_loss: 0.0067 - loss: 0.0067\n",
            "Epoch 43/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 135ms/step - clip_loss: 0.0065 - loss: 0.0065\n",
            "Epoch 44/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - clip_loss: 0.0062 - loss: 0.0062\n",
            "Epoch 45/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - clip_loss: 0.0066 - loss: 0.0066\n",
            "Epoch 46/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 210ms/step - clip_loss: 0.0060 - loss: 0.0060\n",
            "Epoch 47/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - clip_loss: 0.0061 - loss: 0.0061\n",
            "Epoch 48/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - clip_loss: 0.0057 - loss: 0.0057\n",
            "Epoch 49/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 171ms/step - clip_loss: 0.0054 - loss: 0.0054\n",
            "Epoch 50/50\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 191ms/step - clip_loss: 0.0054 - loss: 0.0054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history_simple(history)"
      ],
      "metadata": {
        "id": "ZoJF_B8sQ1j9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "9196f923-ff76-400e-e954-38e299bb615f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(history.history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jozf8QsvPTIf",
        "outputId": "94d5d23b-aebc-475e-86a6-94afc6547b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def save_history_csv(history, filename):\n",
        "    \"\"\"save history as csv file\"\"\"\n",
        "    history_dict = history.history\n",
        "    keys = history_dict.keys() # headers\n",
        "    rows = zip(*history_dict.values())  # values\n",
        "\n",
        "    with open(filename, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(keys)\n",
        "        writer.writerows(rows)\n",
        "\n",
        "history_filename = os.path.join(save_dir, f\"{full_model_name}_history.csv\")\n",
        "save_history_csv(history, history_filename)"
      ],
      "metadata": {
        "id": "joVLzW2ZQ8wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Xmmf23tPL54"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}